[encoder]
vocab_size = 32128
d_model = 1024
d_kv = 128
d_ff = 16384
num_layers = 24
num_decoder_layers = 24
num_heads = 32
relative_attention_num_buckets_or_max_pos_seq_len = 32
dropout_rate = 0.1
layer_norm_epsilon = 1e-06
initializer_factor = 1.0
feed_forward_proj = relu
use_cache = False
return_dict = True
output_hidden_states = False
output_attentions = False
torchscript = False
torch_dtype = None
use_bfloat16 = False
pruned_heads = {}
tie_word_embeddings = True
is_encoder_decoder = False
is_decoder = False
add_cross_attention = False
tie_encoder_decoder = False
max_length = 20
min_length = 0
do_sample = False
early_stopping = False
num_beams = 1
num_beam_groups = 1
diversity_penalty = 0.0
temperature = 1.0
top_k = 50
top_p = 1.0
repetition_penalty = 1.0
length_penalty = 1.0
no_repeat_ngram_size = 0
encoder_no_repeat_ngram_size = 0
bad_words_ids = None
num_return_sequences = 1
chunk_size_feed_forward = 0
output_scores = False
return_dict_in_generate = False
forced_bos_token_id = None
forced_eos_token_id = None
remove_invalid_values = False
architectures = ['T5WithLMHeadModel']
finetuning_task = None
id2label = {0: 'LABEL_0', 1: 'LABEL_1'}
label2id = {'LABEL_0': 0, 'LABEL_1': 1}
tokenizer_class = None
prefix = None
bos_token_id = None
pad_token_id = 0
eos_token_id = 1
sep_token_id = None
decoder_start_token_id = 0
task_specific_params = {'summarization': {'early_stopping': True, 'length_penalty': 2.0, 'max_length': 200, 'min_length': 30, 'no_repeat_ngram_size': 3, 'num_beams': 4, 'prefix': 'summarize: '}, 'translation_en_to_de': {'early_stopping': True, 'max_length': 300, 'num_beams': 4, 'prefix': 'translate English to German: '}, 'translation_en_to_fr': {'early_stopping': True, 'max_length': 300, 'num_beams': 4, 'prefix': 'translate English to French: '}, 'translation_en_to_ro': {'early_stopping': True, 'max_length': 300, 'num_beams': 4, 'prefix': 'translate English to Romanian: '}}
problem_type = None
_name_or_path = /byshiue/data/t5-models/t5-3b/
transformers_version = 4.11.3
model_type = t5
n_positions = 512
output_past = True
weight_data_type = fp32

[decoder]
vocab_size = 32128
d_model = 1024
d_kv = 128
d_ff = 16384
num_layers = 24
num_decoder_layers = 24
num_heads = 32
relative_attention_num_buckets_or_max_pos_seq_len = 32
dropout_rate = 0.1
layer_norm_epsilon = 1e-06
initializer_factor = 1.0
feed_forward_proj = relu
use_cache = True
return_dict = True
output_hidden_states = False
output_attentions = False
torchscript = False
torch_dtype = None
use_bfloat16 = False
pruned_heads = {}
tie_word_embeddings = True
is_encoder_decoder = False
is_decoder = True
add_cross_attention = False
tie_encoder_decoder = False
max_length = 20
min_length = 0
do_sample = False
early_stopping = False
num_beams = 1
num_beam_groups = 1
diversity_penalty = 0.0
temperature = 1.0
top_k = 50
top_p = 1.0
repetition_penalty = 1.0
length_penalty = 1.0
no_repeat_ngram_size = 0
encoder_no_repeat_ngram_size = 0
bad_words_ids = None
num_return_sequences = 1
chunk_size_feed_forward = 0
output_scores = False
return_dict_in_generate = False
forced_bos_token_id = None
forced_eos_token_id = None
remove_invalid_values = False
architectures = ['T5WithLMHeadModel']
finetuning_task = None
id2label = {0: 'LABEL_0', 1: 'LABEL_1'}
label2id = {'LABEL_0': 0, 'LABEL_1': 1}
tokenizer_class = None
prefix = None
bos_token_id = None
pad_token_id = 0
eos_token_id = 1
sep_token_id = None
decoder_start_token_id = 0
task_specific_params = {'summarization': {'early_stopping': True, 'length_penalty': 2.0, 'max_length': 200, 'min_length': 30, 'no_repeat_ngram_size': 3, 'num_beams': 4, 'prefix': 'summarize: '}, 'translation_en_to_de': {'early_stopping': True, 'max_length': 300, 'num_beams': 4, 'prefix': 'translate English to German: '}, 'translation_en_to_fr': {'early_stopping': True, 'max_length': 300, 'num_beams': 4, 'prefix': 'translate English to French: '}, 'translation_en_to_ro': {'early_stopping': True, 'max_length': 300, 'num_beams': 4, 'prefix': 'translate English to Romanian: '}}
problem_type = None
_name_or_path = /byshiue/data/t5-models/t5-3b/
transformers_version = 4.11.3
model_type = t5
n_positions = 512
output_past = True
weight_data_type = fp32

[structure]
t5_with_bias = 0 # 0 = no bias; 1 = has bias
position_embedding_type = 0 # 0 = relative position embedding; 1 = absolute position embedding